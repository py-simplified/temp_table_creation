# Standard library imports
import io
import time
from typing import List, Optional

# Third-party imports
import pandas as pd
import streamlit as st
from google.cloud import bigquery
from google.cloud.bigquery import SchemaField
from google.cloud.bigquery import LoadJobConfig
from google.cloud.bigquery import ScalarQueryParameter, ArrayQueryParameter
from sql_test_creator import render_sql_test_creator_tab

# -----------------------------
# Configuration
# -----------------------------
# --- BigQuery configuration ---
TABLE_ID = "Validation_Scenarios"  # config table name

# Required columns for config table
REQUIRED_COLS = [
    "Source_Project_Id",
    "Source_Dataset_Id",
    "Source_Table",
    "Target_Project_Id",
    "Target_Dataset_Id",
    "Target_Table",
    "Target_Column",
    "Derivation_Logic",
]

# Canonical casing for common columns (case-insensitive mapping)
CANONICAL_MAP = {
    "scenario_id": "Scenario_ID",
    "scenario_name": "Scenario_Name",
    "description": "Description",
    "source_project_id": "Source_Project_Id",
    "source_dataset_id": "Source_Dataset_Id",
    "source_table": "Source_Table",
    "target_project_id": "Target_Project_Id",
    "target_dataset_id": "Target_Dataset_Id",
    "target_table": "Target_Table",
    "target_column": "Target_Column",
    "reference_table": "Reference_Table",
    "reference_join_key": "Reference_Join_Key",
    "reference_lookup_column": "Reference_Lookup_Column",
    "derivation_logic": "Derivation_Logic",
}

# -----------------------------
# Helpers
# -----------------------------

# Returns a cached BigQuery client for efficient repeated queries
@st.cache_resource(show_spinner=False)
def get_bq_client(project_id=None):
    return bigquery.Client(project=project_id)

def _sanitize_col(col: str) -> str:

    # Cleans up column names for consistent mapping
    s = col.strip().replace("\n", " ").replace("-", "_").replace(" ", "_")
    s = "_".join([p for p in s.split("_") if p])  # collapse repeats
    return s

def _canonicalize_headers(df: pd.DataFrame) -> pd.DataFrame:

    # Maps DataFrame columns to canonical names using CANONICAL_MAP
    new_cols = []
    for c in df.columns:
        key = _sanitize_col(c).lower()
        new_cols.append(CANONICAL_MAP.get(key, _sanitize_col(c)))
    df.columns = new_cols
    return df

def ensure_config_table(client: bigquery.Client, df: pd.DataFrame, table_fqn: str):

    # Ensures the config table exists in BigQuery and has all required columns
    try:
        client.get_table(table_fqn)
        table_exists = True
    except Exception:
        table_exists = False

    if not table_exists:
        # Create table if it doesn't exist
        schema = [SchemaField(col, "STRING") for col in df.columns]
        table = bigquery.Table(table_fqn, schema=schema)
        client.create_table(table)
    else:
        # Add missing columns if table exists
        tbl = client.get_table(table_fqn)
        existing_cols = {sch.name.lower() for sch in tbl.schema}
        to_add = [c for c in df.columns if c.lower() not in existing_cols]
        for col in to_add:
            ddl = f"ALTER TABLE `{table_fqn}` ADD COLUMN IF NOT EXISTS `{col}` STRING"
            client.query(ddl).result()

def load_excel_to_bq(excel_bytes: bytes, mode: str, project_id: str, dataset_id: str, table_id: str):

    # Loads the uploaded Excel config into BigQuery
    client = get_bq_client(project_id)
    table_fqn = f"{project_id}.{dataset_id}.{table_id}"
    # Read both worksheets
    xls = pd.ExcelFile(io.BytesIO(excel_bytes), engine="openpyxl")
    df_comp = xls.parse("Component_detail")
    df_cfg = xls.parse("Temp_table_config")
    # Canonicalize headers
    df_comp = _canonicalize_headers(df_comp)
    df_cfg = _canonicalize_headers(df_cfg)
    # Left join on Component
    df = pd.merge(df_cfg, df_comp, on="Component", how="left")

    # Check for missing required columns
    missing = [c for c in REQUIRED_COLS if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    # Ensure config table exists and is up to date
    ensure_config_table(client, df, table_fqn)

    # Set write mode for BigQuery load job
    job_config = LoadJobConfig()
    if mode == "Recreate (Full Refresh)":
        job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE
    elif mode == "Append (Add New Rows)":
        job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND
    else:
        raise ValueError("Invalid load mode")

    # Convert all columns to string for BigQuery
    for col in df.columns:
        df[col] = df[col].astype(str)

    # Load DataFrame to BigQuery
    load_job = client.load_table_from_dataframe(df, table_fqn, job_config=job_config)
    load_job.result()

def list_components(project_id: str, dataset_id: str, table_id: str) -> List[str]:

    # Returns a list of distinct target tables from the config table
    # Only create client when this function is called (i.e., after user action)
    client = get_bq_client(project_id)
    table_fqn = f"{project_id}.{dataset_id}.{table_id}"
    sql = f"SELECT DISTINCT Target_Table FROM `{table_fqn}` WHERE Target_Table IS NOT NULL ORDER BY Target_Table"
    rows = client.query(sql).result()
    return [row["Target_Table"] for row in rows]

def preview_config(project_id: str, dataset_id: str, table_id: str, limit: int = 50) -> pd.DataFrame:

    # Returns a preview of the config table as a DataFrame
    # Only create client when this function is called (i.e., after user action)
    client = get_bq_client(project_id)
    table_fqn = f"{project_id}.{dataset_id}.{table_id}"
    sql = f"SELECT * FROM `{table_fqn}` LIMIT {limit}"
    return client.query(sql).to_dataframe()

# --- CTAS SQL Preview Helper ---
def generate_ctas_sqls(config_df: pd.DataFrame, selected_targets: Optional[List[str]] = None) -> List[str]:

    # Generates the SQL for creating temp tables for each selected target
    ctas_sqls = []
    if config_df.empty:
        return ctas_sqls
    # Filter targets based on selection
    if selected_targets:
        targets_df = config_df[config_df["Target_Table"].isin(selected_targets)]
    else:
        targets_df = config_df[config_df["Target_Table"].notnull()]
    # Group by each target+source combination (allows same target table from different source datasets)
    for (proj, ds, tbl, src_proj, src_ds, src_tbl), group in targets_df.groupby(["Target_Project_Id", "Target_Dataset_Id", "Target_Table", "Source_Project_Id", "Source_Dataset_Id", "Source_Table"]):
        # Guardrail #1: within this target+source combination, should have exactly one source (already guaranteed by groupby)
        sources = group[["Source_Project_Id", "Source_Dataset_Id", "Source_Table"]].drop_duplicates()
        if len(sources) != 1:
            ctas_sqls.append({"table": f"{tbl}_{src_ds}", "sql": f"-- ERROR: Target {tbl} from {src_ds} maps to {len(sources)} sources, expected 1."})
            continue
        source = sources.iloc[0]
        # Guardrail #2: no duplicate target columns within this target+source combination
        dup_cols = group["Target_Column"].duplicated().sum()
        if dup_cols > 0:
            ctas_sqls.append({"table": f"{tbl}_{src_ds}", "sql": f"-- ERROR: Target {tbl} from {src_ds} has duplicate columns."})
            continue
        # --- Reference JOIN planning ---
        import re
        ref_tables = group[group["Reference_Table"].notnull() & (group["Reference_Table"].str.lower() != "nan")]["Reference_Table"].drop_duplicates().tolist()
        ref_pattern = "|".join([re.escape(str(r)) for r in ref_tables]) if ref_tables else ""
        join_clauses = []
        ref_alias_counter = {}  # Track aliases for each reference table to ensure uniqueness
        
        # Create a mapping from (ref_table, join_key, lookup_col) to unique alias
        ref_combinations = group[group["Reference_Table"].notnull() & (group["Reference_Table"].str.lower() != "nan") & group["Reference_Join_Key"].notnull() & group["Reference_Lookup_Column"].notnull()][["Reference_Table", "Reference_Join_Key", "Reference_Lookup_Column"]].drop_duplicates()
        ref_alias_map = {}
        
        for _, ref in ref_combinations.iterrows():
            ref_tbl = str(ref["Reference_Table"])
            join_key = str(ref["Reference_Join_Key"])
            lookup_col = str(ref["Reference_Lookup_Column"])
            
            # Create unique alias for this combination
            if ref_tbl not in ref_alias_counter:
                ref_alias_counter[ref_tbl] = 0
            ref_alias_counter[ref_tbl] += 1
            
            if ref_alias_counter[ref_tbl] == 1:
                alias = ref_tbl  # First occurrence uses table name as alias
            else:
                alias = f"{ref_tbl}_{ref_alias_counter[ref_tbl]}"  # Subsequent uses numbered alias
            
            # Store mapping for later use in derivation logic
            ref_alias_map[(ref_tbl, join_key, lookup_col)] = alias
            
            full_ref_fqn = f"{source['Source_Project_Id']}.{source['Source_Dataset_Id']}.{ref_tbl}"
            join_clauses.append(
                f"LEFT JOIN `{full_ref_fqn}` AS `{alias}` ON `{source['Source_Table']}`.`{lookup_col}` = `{alias}`.`{join_key}`"
            )
        join_sql = "\n".join(join_clauses)

        # --- Build SELECT list with reference fixups ---
        def fix_derivation(row):
            logic = str(row["Derivation_Logic"])
            ref_tbl = str(row["Reference_Table"])
            ref_join_key = str(row["Reference_Join_Key"])
            ref_lookup_col = str(row["Reference_Lookup_Column"])
            
            if ref_tbl and ref_tbl.lower() != "nan" and ref_pattern:
                # Find the correct alias for this specific reference combination
                alias_key = (ref_tbl, ref_join_key, ref_lookup_col)
                if alias_key in ref_alias_map:
                    correct_alias = ref_alias_map[alias_key]
                    logic = re.sub(r"\bref\.", f"{correct_alias}.", logic)
                    logic = re.sub(rf"\b{re.escape(ref_tbl)}\.", f"{correct_alias}.", logic)
            return logic

        group_sorted = group.sort_values(["Scenario_ID", "Target_Column"])
        projection = ",\n  ".join([
            f"{fix_derivation(row)} AS `{str(row['Target_Column']).replace('`','')}`" for _, row in group_sorted.iterrows()
        ])
        if not projection:
            ctas_sqls.append({"table": tbl, "sql": f"-- ERROR: Target {tbl} has no columns to select."})
            continue

        # --- Final CTAS SQL ---
                # New temp table name: <Target_Table>_<Source_Dataset_Id>_Temp
        temp_table_name = f"{tbl}_Temp"
        ctas = f"""
        CREATE OR REPLACE TABLE `{proj}.{ds}.{temp_table_name}` AS
        SELECT
            {projection}
        FROM `{source['Source_Project_Id']}.{source['Source_Dataset_Id']}.{source['Source_Table']}` AS `{source['Source_Table']}`
        {join_sql}
        """
        ctas_sqls.append({"table": temp_table_name, "sql": ctas.strip(), "project": proj, "dataset": ds})
    return ctas_sqls

    # --- CTAS SQL Preview Helper ---
    def generate_ctas_sqls(config_df: pd.DataFrame, selected_targets: Optional[List[str]] = None) -> List[str]:
        ctas_sqls = []
        if config_df.empty:
            return ctas_sqls
        # Filter targets
        if selected_targets:
            targets_df = config_df[config_df["Target_Table"].isin(selected_targets)]
        else:
            targets_df = config_df[config_df["Target_Table"].notnull()]
        # Group by target+source combination (allows same target table from different source datasets)
        for (proj, ds, tbl, src_proj, src_ds, src_tbl), group in targets_df.groupby(["Target_Project_Id", "Target_Dataset_Id", "Target_Table", "Source_Project_Id", "Source_Dataset_Id", "Source_Table"]):
            # Guardrail #1: within this target+source combination, should have exactly one source (already guaranteed by groupby)
            sources = group[["Source_Project_Id", "Source_Dataset_Id", "Source_Table"]].drop_duplicates()
            if len(sources) != 1:
                ctas_sqls.append(f"-- ERROR: Target {tbl} from {src_ds} maps to {len(sources)} sources, expected 1.")
                continue
            source = sources.iloc[0]
            # Guardrail #2: no duplicate target columns within this target+source combination
            dup_cols = group["Target_Column"].duplicated().sum()
            if dup_cols > 0:
                ctas_sqls.append(f"-- ERROR: Target {tbl} from {src_ds} has duplicate columns.")
                continue
            # Build projection
            group_sorted = group.sort_values(["Scenario_ID", "Target_Column"])
            projection = ",\n  ".join([
                f"{row['Derivation_Logic']} AS `{row['Target_Column']}" for _, row in group_sorted.iterrows()
            ])
            if not projection:
                ctas_sqls.append(f"-- ERROR: Target {tbl} from {src_ds} has no columns to select.")
                continue
            # Build CTAS with source dataset in table name
            temp_table_name = f"{tbl}_Temp"
            ctas = f"""
    CREATE OR REPLACE TABLE `{proj}.{ds}.{temp_table_name}` AS
SELECT
  {projection}
FROM `{source['Source_Project_Id']}.{source['Source_Dataset_Id']}.{source['Source_Table']}` AS `{source['Source_Table']}`
    """
            ctas_sqls.append(ctas.strip())
        return ctas_sqls

def call_build_proc(selected: Optional[List[str]], project_id: str, dataset_id: str, proc_name: str, table_id: str):
    client = get_bq_client(project_id)
    proc_fqn = f"{project_id}.{dataset_id}.{proc_name}"
    table_fqn = f"{project_id}.{dataset_id}.{table_id}"
    sql = f"CALL `{proc_fqn}`(@cfg, @targets)"
    params = [
        ScalarQueryParameter("cfg", "STRING", table_fqn),
        ArrayQueryParameter("targets", "STRING", selected if selected else []),
    ]
    job = client.query(sql, job_config=bigquery.QueryJobConfig(query_parameters=params))
    job.result()

def post_procedure_steps():
    # Robust demo notification to make the post-procedure hook unmistakable in the UI.
    ts = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
    msg = f"Post-procedure steps completed successfully! (Demo message) — {ts}"
    # Keep a small session log so user can inspect if messages were shown
    if "post_proc_log" not in st.session_state:
        st.session_state["post_proc_log"] = []
    st.session_state["post_proc_log"].append(msg)
    # Visible messages
    st.success(msg)
    # Looker Studio report (open in new tab). We attempt a best-effort auto-open via JS;
    # browsers may block automatic window.open — the clickable link is always available.
    looker_url = "https://lookerstudio.google.com/reporting/9f6bc074-9eed-4150-8638-3b61d2bb609f"
    # Prominent clickable link
    st.markdown(
        f'<a href="{looker_url}" target="_blank" rel="noopener noreferrer" '
        f'style="font-weight:600; background:#1f77b4; color:white; padding:8px 12px; '
        f'border-radius:6px; text-decoration:none;">Open Looker Studio report</a>',
        unsafe_allow_html=True,
    )
    # Best-effort auto-open (may be blocked by popup blockers). This is harmless if blocked.
    st.markdown(
        f'<script>try{{window.open("{looker_url}", "_blank");}}catch(e){{console.log("popup blocked",e);}}</script>',
        unsafe_allow_html=True,
    )
    # Also print to console (useful when running locally)
    print(msg)

# -----------------------------
# UI
# -----------------------------

# --- Streamlit UI setup ---

# --- Tabs UI ---
st.set_page_config(page_title="Validation Scenarios Manager", layout="wide")
tabs = st.tabs(["ETL - Test Table Creator", "SQL - Test Creator", "RAFT Comparison tool"])


# Tab 1: Temporary Table Creator (existing UI)
with tabs[0]:
    # st.title("Validation Scenarios Manager")
    
    # BigQuery Configuration for Temp Table Creator - moved to main area
    st.header("ETL - Test Table Creator")
    col1, col2, col3 = st.columns(3)
    with col1:
        PROJECT_ID = st.text_input("Project ID", value="project-id", key="temp_project_id")
    with col2:
        DATASET_ID = st.text_input("Dataset ID", value="dataset-id", key="temp_dataset_id")
    with col3:
        PROC_NAME = st.text_input("Stored Procedure Name", value="stored_procedure_name", key="temp_proc_name")
    
    TABLE_FQN = f"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}"
    PROC_FQN = f"{PROJECT_ID}.{DATASET_ID}.{PROC_NAME}"
    
    st.caption(f"Config table: `{TABLE_FQN}` · Procedure: `{PROC_FQN}`")

    with st.expander("1) Upload or Update Config (Excel → BigQuery)", expanded=False):
        # Section 1: Upload Excel config to BigQuery
        uploaded = st.file_uploader("Upload Validation_Scenarios Excel (.xlsx)", type=["xlsx"])
        mode = st.radio("Upload mode", ["Recreate (Full Refresh)", "Append (Add New Rows)"], index=0, horizontal=True, key="upload_mode_temp_tab")
        if st.button("Upload to BigQuery", type="primary", disabled=uploaded is None, key="upload_btn_temp_tab"):
            if not uploaded:
                st.error("Please upload an Excel file.")
            else:
                try:
                    with st.spinner("Uploading..."):
                        load_excel_to_bq(uploaded.getvalue(), mode, PROJECT_ID, DATASET_ID, TABLE_ID)
                    st.success(f"Upload complete with mode: {mode}")
                    st.session_state['upload_success'] = True
                except Exception as e:
                    st.exception(e)
                    st.session_state['upload_success'] = False

    # Initialize selected as empty list
    selected = []

    with st.expander("2) Components Selection", expanded=False):
        # Section 2: Select target tables and preview config
        if uploaded and st.session_state.get('upload_success', False):
            try:
                components = list_components(PROJECT_ID, DATASET_ID, TABLE_ID)
                select_all = st.checkbox("Select All", value=False)
                default = components if select_all else []
                selected = st.multiselect("Target tables", components, default=default)

                st.write("Preview (first 50 rows):")
                try:
                    df_prev = preview_config(PROJECT_ID, DATASET_ID, TABLE_ID, 50)
                    st.dataframe(df_prev, use_container_width=True, hide_index=True)
                except Exception:
                    st.info("Config table preview not available yet.")
            except Exception:
                st.info("Config table not found. Upload the Excel first.")
                selected = []
        else:
            st.info("Upload the Excel config to enable component selection.")

    # --- Review Temp Table SQLs expander (before Execute) ---
    with st.expander("3) Review Temp Table SQLs", expanded=False):
        # Section 3: Preview SQL only
        if uploaded and st.session_state.get('upload_success', False) and selected:
            try:
                config_df = preview_config(PROJECT_ID, DATASET_ID, TABLE_ID, 1000)  # get all config rows
                ctas_sqls = generate_ctas_sqls(config_df, selected)
                st.subheader("Preview: Create Table As Select SQL for Selected Targets")
                if ctas_sqls:
                    for item in ctas_sqls:
                        proj_show = item.get('project', PROJECT_ID) # type: ignore
                        ds_show = item.get('dataset', DATASET_ID) # pyright: ignore[reportAttributeAccessIssue]
                        st.markdown(f"### Project: `{proj_show}` · Dataset: `{ds_show}` · Target Table: `{item['table']}`")

                        st.code(item['sql'], language="sql")
                else:
                    st.info("No SQL to preview. Check your selection or config table.")
            except Exception as e:
                st.warning(f"Could not generate SQL preview: {e}")
        else:
            st.info("Upload the Excel config and select components to preview SQL.")

    # --- Stored procedure expander: expanded only if components are selected ---
    with st.expander("4) Execute Stored Procedure", expanded=bool(selected and len(selected) > 0)):
        if selected and len(selected) > 0:
            # Dedicated placeholder for post-procedure message to guarantee visible placement
            post_placeholder_etl = st.empty()
            st.subheader("Stored Procedure Command to be Executed")
            targets_str = ', '.join([f"'{t}'" for t in selected])
            targets_array = f"[{targets_str}]"
            proc_cmd = f"CALL `{PROC_FQN}`('{TABLE_FQN}', {targets_array});"
            st.markdown(f"""
<div style='overflow-x:auto; background:#f8f9fa; border-radius:8px; padding:8px;'>
<pre style='white-space:pre-wrap; word-break:break-all;'>
{proc_cmd}
</pre>
</div>
""", unsafe_allow_html=True)

            # Calculate and display total time taken instead of start and finish times
            if st.button("Run Stored Procedure", type="primary", key="run_proc_btn_temp_tab"):
                try:
                    start = time.time()
                    with st.spinner("Running stored procedure..."):
                        call_build_proc(selected, PROJECT_ID, DATASET_ID, PROC_NAME, TABLE_ID)
                    end = time.time()
                    total_duration = end - start
                    hours = int(total_duration // 3600)
                    minutes = int((total_duration % 3600) // 60)
                    seconds = int(total_duration % 60)
                    time_str = []
                    if hours > 0:
                        time_str.append(f"{hours} hour{'s' if hours != 1 else ''}")
                    if minutes > 0:
                        time_str.append(f"{minutes} minute{'s' if minutes != 1 else ''}")
                    time_str.append(f"{seconds} second{'s' if seconds != 1 else ''}")
                    st.success(f"Process completed. Total time taken: {', '.join(time_str)}")
                    # Run post-procedure steps (demo hook) before showing the last log so the newest
                    # message is presented to the user immediately (mirrors RAFT behavior).
                    post_procedure_steps()
                    # Surface the last post-proc log entry explicitly into the dedicated placeholder
                    if st.session_state.get("post_proc_log"):
                        last = st.session_state["post_proc_log"][-1]
                        post_placeholder_etl.success(last)
                    st.code(proc_cmd, language="sql")
                except Exception as e:
                    st.exception(e)
        else:
            st.info("Please select at least one component from the Components Selection section above.")

    # --- New expander for sample data preview ---
    show_data = st.checkbox("Show 5 rows from each created temp table (after procedure run)")
    if show_data and selected:
        with st.expander("Sample Data from Created Temp Tables", expanded=True):
            client = get_bq_client(PROJECT_ID)
            config_df = preview_config(PROJECT_ID, DATASET_ID, TABLE_ID, 1000)
            ctas_sqls = generate_ctas_sqls(config_df, selected)
            for item in ctas_sqls:
                temp_table = item['table'] if str(item['table']).endswith('_Temp') else f"{item['table']}_Temp"
                try:
                    # Normalize base (remove trailing _Temp) and search config in two steps:
                    # 1) direct Target_Table == base
                    # 2) fallback to Target_Table + '_' + Source_Dataset_Id == base
                    if temp_table.endswith('_Temp'):
                        base = temp_table[:-5]  # remove '_Temp'
                        base_norm = base.strip().lower()

                        # Step 1: direct Target_Table match (normalized)
                        match = config_df[
                            config_df.apply(
                                lambda row: str(row.get('Target_Table', '')).strip().lower() == base_norm,
                                axis=1
                            )
                        ]

                        # Step 2: fallback to Target_Table + '_' + Source_Dataset_Id
                        if match.empty:
                            match = config_df[
                                config_df.apply(
                                    lambda row: f"{str(row.get('Target_Table','')).strip()}_{str(row.get('Source_Dataset_Id','')).strip()}".lower() == base_norm,
                                    axis=1
                                )
                            ]

                        if not match.empty:
                            proj = match.iloc[0]['Target_Project_Id']
                            ds = match.iloc[0]['Target_Dataset_Id']
                            temp_fqn = f"{proj}.{ds}.{temp_table}"
                            sql = f"SELECT * FROM `{temp_fqn}` LIMIT 5"
                            st.markdown(f"**{temp_fqn}**")
                            st.code(sql, language="sql")
                            df = client.query(sql).to_dataframe()
                            st.dataframe(df)
                        else:
                            st.warning(f"Could not find config for temp table {temp_table}")
                    else:
                        st.warning(f"Temp table name does not end with _Temp: {temp_table}")
                except Exception as e:
                    st.error(f"Error fetching data from {temp_table}: {e}")

    # Footer note for authentication
    st.divider()
    st.caption("Note: Uses Application Default Credentials. Locally: `gcloud auth application-default login`.")

with tabs[1]:  # SQL - Test Creator
    render_sql_test_creator_tab()

# Tab 2: RAFT Comparison tool (structure only, placeholder logic)
with tabs[2]:
    # st.title("RAFT Comparison tool")
    
    # RAFT BigQuery Configuration - moved to main area
    st.header("RAFT Comparison tool")
    col1, col2 = st.columns(2)
    with col1:
        RAFT_PROJECT_ID = st.text_input("RAFT Project ID", value="raft-project-id", key="raft_project_id")
    with col2:
        RAFT_DATASET_ID = st.text_input("RAFT Dataset ID", value="raft-dataset-id", key="raft_dataset_id")

    with st.expander("1) Upload or Update RAFT Config (Excel → BigQuery)", expanded=False):
        raft_excel_file = st.file_uploader("Upload RAFT Config Excel (.xlsx)", type=["xlsx"], key="raft_config_excel")
        selected_sheets = []
        sheet_names = []
        if raft_excel_file:
            try:
                import openpyxl
                xls = pd.ExcelFile(raft_excel_file, engine="openpyxl")
                sheet_names = xls.sheet_names
                selected_sheets = st.multiselect("Select worksheets to upload as tables in BigQuery:", sheet_names, default=sheet_names)
                upload_mode = st.radio("Upload mode", ["Recreate (Full Refresh)", "Append (Add New Rows)"], index=0, horizontal=True, key="upload_mode_raft_tab")
                if st.button("Upload selected worksheets to BigQuery", type="primary", key="upload_btn_raft_excel"):
                    client = get_bq_client(RAFT_PROJECT_ID)
                    for sheet in selected_sheets:
                        df = xls.parse(sheet)
                        table_fqn = f"{RAFT_PROJECT_ID}.{RAFT_DATASET_ID}.{sheet}"
                        job_config = bigquery.LoadJobConfig()
                        if upload_mode == "Recreate (Full Refresh)":
                            job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE
                        else:
                            job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND
                        # Preserve empty cells as empty strings (''), and ensure all columns are strings
                        # This avoids pandas converting NaN to the literal string 'nan' while keeping
                        # the destination columns as STRING in BigQuery.
                        df = df.fillna("").astype(str)
                        with st.spinner(f"Uploading {sheet} to {table_fqn}..."):
                            client.load_table_from_dataframe(df, table_fqn, job_config=job_config).result()
                    st.success(f"Uploaded {', '.join(selected_sheets)} to BigQuery.")
            except Exception as e:
                st.error(f"Error reading Excel file: {e}")

    with st.expander("2) Components Selection", expanded=False):
        # Only load components after user clicks button
        load_components = st.button("Load Components from RAFT_MASTER_CONFIG", type="primary", key="load_raft_components_btn")
        
        # Initialize components from session state or empty list
        if 'raft_components' not in st.session_state:
            st.session_state.raft_components = []
        
        if load_components:
            client = get_bq_client(RAFT_PROJECT_ID)
            raft_master_table = f"{RAFT_PROJECT_ID}.{RAFT_DATASET_ID}.RAFT_MASTER_CONFIG"
            try:
                df_master = client.query(f"SELECT Component FROM `{raft_master_table}`").to_dataframe()
                st.session_state.raft_components = df_master['Component'].dropna().unique().tolist()
            except Exception as e:
                st.warning(f"Could not load RAFT_MASTER_CONFIG: {e}")
        
        components = st.session_state.raft_components

        # Stored procedures
        raft_procs = [
            "RAFT_AGG_BAL_CMPR_SP",
            "RAFT_DATA_CMPR_SP",
            "RAFT_METADATA_CMPR_SP",
            "RAFT_REFDATA_CMPR_SP"
        ]

        # Select All checkbox (outside table)
        select_all = st.checkbox("Select All Components & Procedures", key="raft_select_all")

        if components:
            # Multi-select dropdown for components
            selected_components = st.multiselect(
                "Select Components",
                options=components,
                default=components if select_all else []
            )

            # Multi-select dropdown for procedures
            selected_procedures = st.multiselect(
                "Select Procedures",
                options=raft_procs,
                default=raft_procs if select_all else []
            )

            # Generate a DataFrame based on selected components and procedures
            if selected_components and selected_procedures:
                df_editor = pd.DataFrame(
                    index=selected_components,
                    columns=selected_procedures
                )
                df_editor.loc[:, :] = True  # Mark all selected combinations as True

                # Display the DataFrame
                st.dataframe(df_editor, use_container_width=True)
            else:
                st.info("Please select at least one component and one procedure.")
                
        elif load_components:
            st.info("No components found in RAFT_MASTER_CONFIG table.")
        else:
            st.info("Click 'Load Components from RAFT_MASTER_CONFIG' to view components.")

    with st.expander("3) Review RAFT Stored Procedures call", expanded=False):
        generate_calls = st.button("Generate CALL Commands", type="primary", key="generate_raft_calls_btn")
        # Use the return value of st.data_editor from the previous section
        if generate_calls:
            if df_editor is not None and len(df_editor) > 0:
                call_commands = []
                for component in df_editor.index:
                    component_calls = []
                    for proc in df_editor.columns:
                        if df_editor.loc[component, proc]:
                            # Determine argument list based on procedure name
                            quoted_component = f'"{component}"'
                            if proc == "RAFT_REFDATA_CMPR_SP" or proc == "RAFT_METADATA_CMPR_SP":
                                args = ', '.join([quoted_component, '""', '""'])
                            elif proc == "RAFT_DATA_CMPR_SP":
                                args = ', '.join([quoted_component, '""', '""', '""', '""'])
                            elif proc == "RAFT_AGG_BAL_CMPR_SP":
                                args = ', '.join([quoted_component, '""', '""', '""'])
                            else:
                                args = quoted_component
                            call_cmd = f"CALL `{RAFT_PROJECT_ID}.{RAFT_DATASET_ID}.{proc}`({args});"
                            component_calls.append(call_cmd)
                    if component_calls:
                        call_commands.append({
                            'component': component,
                            'calls': component_calls
                        })
                if call_commands:
                    st.subheader("All Commands (Grouped by Component)")
                    for item in call_commands:
                        st.markdown(f"#### Component: `{item['component']}`")
                        st.code("\n".join(item['calls']), language="sql")
                else:
                    st.info("No procedures selected. Please select components and procedures.")
        elif generate_calls:
            st.info("Please load components and make selections in the Components Selection section first.")
        else:
            st.info("Click 'Generate CALL Commands' to preview stored procedure calls based on your selections.")

    with st.expander("4) Execute RAFT Stored Procedure", expanded=False):
        batch_size = st.selectbox("How many stored procedures to run in parallel?", options=list(range(1, 11)), index=0, key="raft_batch_size")
        run_calls = st.button("Run Selected Stored Procedures", type="primary", key="run_raft_calls_btn")
        # Use the edited_df from the data editor above
        if run_calls and 'df_editor' in locals():
            if df_editor is not None and len(df_editor) > 0:
                call_commands = []
                for component in df_editor.index:
                    for proc in raft_procs:
                        if proc in df_editor.columns and df_editor.loc[component, proc]:
                            quoted_component = f'"{component}"'
                            if proc == "RAFT_REFDATA_CMPR_SP" or proc == "RAFT_METADATA_CMPR_SP":
                                args = ', '.join([quoted_component, '""', '""'])
                            elif proc == "RAFT_DATA_CMPR_SP":
                                args = ', '.join([quoted_component, '""', '""', '""', '""'])
                            elif proc == "RAFT_AGG_BAL_CMPR_SP":
                                args = ', '.join([quoted_component, '""', '""', '""'])
                            else:
                                args = quoted_component
                            call_cmd = f"CALL `{RAFT_PROJECT_ID}.{RAFT_DATASET_ID}.{proc}`({args})"
                            call_commands.append({
                                'component': component,
                                'procedure': proc,
                                'command': call_cmd
                            })
                if call_commands:
                    st.subheader("Stored Procedure Execution Summary (Live)")
                    client = get_bq_client(RAFT_PROJECT_ID)
                    import concurrent.futures
                    import time
                    import pandas as pd
                    summary = []
                    # Use a Streamlit placeholder for the summary table
                    table_placeholder = st.empty()
                    # Dedicated placeholder for post-procedure message to guarantee visible placement
                    post_placeholder = st.empty()
                    process_start = time.time()
                    # Run in batches
                    for i in range(0, len(call_commands), batch_size):
                        batch = call_commands[i:i+batch_size]
                        futures = []
                        with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
                            for cmd_info in batch:
                                cmd = cmd_info['command']
                                start = time.time()
                                future = executor.submit(client.query, cmd)
                                futures.append((future, cmd_info, start))
                            for future, cmd_info, start in futures:
                                cmd = cmd_info['command']
                                status = "Completed"
                                error_msg = ""
                                try:
                                    future.result().result()
                                except Exception as e:
                                    status = "Failed"
                                    error_msg = str(e)
                                end = time.time()
                                duration = end - start
                                summary.append({
                                    'Component': cmd_info['component'],
                                    'Procedure': cmd_info['procedure'],
                                    'Start Time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start)),
                                    'End Time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end)),
                                    'Duration': f"{int(duration//3600):02d}:{int((duration%3600)//60):02d}:{int(duration%60):02d}",
                                    'Status': status,
                                    'Error': error_msg
                                })
                        # Update the summary table in-place after each batch
                        table_placeholder.dataframe(pd.DataFrame(summary))
                    # After all batches, show final message and total time
                    process_end = time.time()
                    total_duration = process_end - process_start
                    hours = int(total_duration // 3600)
                    minutes = int((total_duration % 3600) // 60)
                    seconds = int(total_duration % 60)
                    time_str = []
                    if hours > 0:
                        time_str.append(f"{hours} hour{'s' if hours != 1 else ''}")
                    if minutes > 0:
                        time_str.append(f"{minutes} minute{'s' if minutes != 1 else ''}")
                    time_str.append(f"{seconds} second{'s' if seconds != 1 else ''}")
                    st.success(f"Process completed. Total time taken: {', '.join(time_str)}")
                    # Run post-procedure steps (demo hook) for RAFT path
                    post_procedure_steps()
                    # Also write the last post-procedure demo message into the dedicated placeholder
                    if st.session_state.get("post_proc_log"):
                        last = st.session_state["post_proc_log"][-1]
                        post_placeholder.success(last)
                else:
                    st.info("No procedures selected. Please select checkboxes in the Components Selection section.")
            else:
                st.info("No data available. Please load components and make selections first.")
        elif run_calls:
            st.info("Please load components and make selections in the Components Selection section first.")
        else:
            st.info("Select procedures and click 'Run Selected Stored Procedures' to execute them.")

    # Removed checkbox for showing 5 rows from each created RAFT temp table
    st.divider()
    st.caption("Note: Uses Application Default Credentials. Locally: `gcloud auth application-default login`.")
