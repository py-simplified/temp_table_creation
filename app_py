# Standard library imports
import io
import time
from typing import List, Optional

# Third-party imports
import pandas as pd
import streamlit as st
from google.cloud import bigquery
from google.cloud.bigquery import SchemaField
from google.cloud.bigquery import LoadJobConfig
from google.cloud.bigquery import ScalarQueryParameter, ArrayQueryParameter

# -----------------------------
# Configuration
# -----------------------------
# --- BigQuery configuration ---
PROJECT_ID = "cohesive-apogee-411113"
DATASET_ID = "banking_sample_data"
TABLE_ID = "Validation_Scenarios"  # config table name
TABLE_FQN = f"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}"
PROC_FQN = f"{PROJECT_ID}.{DATASET_ID}.build_temp_targets"

# Required columns for config table
REQUIRED_COLS = [
    "Source_Project_Id",
    "Source_Dataset_Id",
    "Source_Table",
    "Target_Project_Id",
    "Target_Dataset_Id",
    "Target_Table",
    "Target_Column",
    "Derivation_Logic",
]

# Canonical casing for common columns (case-insensitive mapping)
CANONICAL_MAP = {
    "scenario_id": "Scenario_ID",
    "scenario_name": "Scenario_Name",
    "description": "Description",
    "source_project_id": "Source_Project_Id",
    "source_dataset_id": "Source_Dataset_Id",
    "source_table": "Source_Table",
    "target_project_id": "Target_Project_Id",
    "target_dataset_id": "Target_Dataset_Id",
    "target_table": "Target_Table",
    "target_column": "Target_Column",
    "reference_table": "Reference_Table",
    "reference_join_key": "Reference_Join_Key",
    "reference_lookup_column": "Reference_Lookup_Column",
    "derivation_logic": "Derivation_Logic",
}

# -----------------------------
# Helpers
# -----------------------------

# Returns a cached BigQuery client for efficient repeated queries
@st.cache_resource(show_spinner=False)
def get_bq_client():
    return bigquery.Client(project=PROJECT_ID)

def _sanitize_col(col: str) -> str:

    # Cleans up column names for consistent mapping
    s = col.strip().replace("\n", " ").replace("-", "_").replace(" ", "_")
    s = "_".join([p for p in s.split("_") if p])  # collapse repeats
    return s

def _canonicalize_headers(df: pd.DataFrame) -> pd.DataFrame:

    # Maps DataFrame columns to canonical names using CANONICAL_MAP
    new_cols = []
    for c in df.columns:
        key = _sanitize_col(c).lower()
        new_cols.append(CANONICAL_MAP.get(key, _sanitize_col(c)))
    df.columns = new_cols
    return df

def ensure_config_table(client: bigquery.Client, df: pd.DataFrame):

    # Ensures the config table exists in BigQuery and has all required columns
    try:
        client.get_table(TABLE_FQN)
        table_exists = True
    except Exception:
        table_exists = False

    if not table_exists:
        # Create table if it doesn't exist
        schema = [SchemaField(col, "STRING") for col in df.columns]
        table = bigquery.Table(TABLE_FQN, schema=schema)
        client.create_table(table)
    else:
        # Add missing columns if table exists
        tbl = client.get_table(TABLE_FQN)
        existing_cols = {sch.name.lower() for sch in tbl.schema}
        to_add = [c for c in df.columns if c.lower() not in existing_cols]
        for col in to_add:
            ddl = f"ALTER TABLE `{TABLE_FQN}` ADD COLUMN IF NOT EXISTS `{col}` STRING"
            client.query(ddl).result()

def load_excel_to_bq(excel_bytes: bytes, mode: str):

    # Loads the uploaded Excel config into BigQuery
    client = get_bq_client()
    # Read both worksheets
    xls = pd.ExcelFile(io.BytesIO(excel_bytes), engine="openpyxl")
    df_comp = xls.parse("Component_detail")
    df_cfg = xls.parse("Temp_table_config")
    # Canonicalize headers
    df_comp = _canonicalize_headers(df_comp)
    df_cfg = _canonicalize_headers(df_cfg)
    # Left join on Component
    df = pd.merge(df_cfg, df_comp, on="Component", how="left")

    # Check for missing required columns
    missing = [c for c in REQUIRED_COLS if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    # Ensure config table exists and is up to date
    ensure_config_table(client, df)

    # Set write mode for BigQuery load job
    job_config = LoadJobConfig()
    if mode == "Recreate (Full Refresh)":
        job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE
    elif mode == "Append (Add New Rows)":
        job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND
    else:
        raise ValueError("Invalid load mode")

    # Convert all columns to string for BigQuery
    for col in df.columns:
        df[col] = df[col].astype(str)

    # Load DataFrame to BigQuery
    load_job = client.load_table_from_dataframe(df, TABLE_FQN, job_config=job_config)
    load_job.result()

def list_components() -> List[str]:

    # Returns a list of distinct target tables from the config table
    client = get_bq_client()
    sql = f"SELECT DISTINCT Target_Table FROM `{TABLE_FQN}` WHERE Target_Table IS NOT NULL ORDER BY Target_Table"
    rows = client.query(sql).result()
    return [row["Target_Table"] for row in rows]

def preview_config(limit: int = 50) -> pd.DataFrame:

    # Returns a preview of the config table as a DataFrame
    client = get_bq_client()
    sql = f"SELECT * FROM `{TABLE_FQN}` LIMIT {limit}"
    return client.query(sql).to_dataframe()

# --- CTAS SQL Preview Helper ---
def generate_ctas_sqls(config_df: pd.DataFrame, selected_targets: Optional[List[str]] = None) -> List[str]:

    # Generates the SQL for creating temp tables for each selected target
    ctas_sqls = []
    if config_df.empty:
        return ctas_sqls
    # Filter targets based on selection
    if selected_targets:
        targets_df = config_df[config_df["Target_Table"].isin(selected_targets)]
    else:
        targets_df = config_df[config_df["Target_Table"].notnull()]
    # Group by each target table
    for (proj, ds, tbl), group in targets_df.groupby(["Target_Project_Id", "Target_Dataset_Id", "Target_Table"]):
        # Guardrail #1: must map to exactly one source
        sources = group[["Source_Project_Id", "Source_Dataset_Id", "Source_Table"]].drop_duplicates()
        if len(sources) != 1:
            ctas_sqls.append({"table": tbl, "sql": f"-- ERROR: Target {tbl} maps to {len(sources)} sources, expected 1."})
            continue
        source = sources.iloc[0]
        # Guardrail #2: no duplicate target columns
        dup_cols = group["Target_Column"].duplicated().sum()
        if dup_cols > 0:
            ctas_sqls.append({"table": tbl, "sql": f"-- ERROR: Target {tbl} has duplicate columns."})
            continue
        # --- Reference JOIN planning ---
        import re
        ref_tables = group[group["Reference_Table"].notnull() & (group["Reference_Table"].str.lower() != "nan")]["Reference_Table"].drop_duplicates().tolist()
        ref_pattern = "|".join([re.escape(str(r)) for r in ref_tables]) if ref_tables else ""
        join_clauses = []
        for _, ref in group[group["Reference_Table"].notnull() & (group["Reference_Table"].str.lower() != "nan") & group["Reference_Join_Key"].notnull() & group["Reference_Lookup_Column"].notnull()][["Reference_Table", "Reference_Join_Key", "Reference_Lookup_Column"]].drop_duplicates().iterrows():
            ref_tbl = str(ref["Reference_Table"])
            full_ref_fqn = f"{source['Source_Project_Id']}.{source['Source_Dataset_Id']}.{ref_tbl}"
            join_clauses.append(
                f"LEFT JOIN `{full_ref_fqn}` AS `{ref_tbl}` ON `{source['Source_Table']}`.`{ref['Reference_Lookup_Column']}` = `{ref_tbl}`.`{ref['Reference_Join_Key']}`"
            )
        join_sql = "\n".join(join_clauses)

        # --- Build SELECT list with reference fixups ---
        def fix_derivation(row):
            logic = str(row["Derivation_Logic"])
            ref_tbl = str(row["Reference_Table"])
            if ref_tbl and ref_tbl.lower() != "nan" and ref_pattern:
                logic = re.sub(r"\bref\.", f"{ref_tbl}.", logic)
                logic = re.sub(rf"\b({ref_pattern})\.", f"{ref_tbl}.", logic)
            return logic

        group_sorted = group.sort_values(["Scenario_ID", "Target_Column"])
        projection = ",\n  ".join([
            f"{fix_derivation(row)} AS `{str(row['Target_Column']).replace('`','')}`" for _, row in group_sorted.iterrows()
        ])
        if not projection:
            ctas_sqls.append({"table": tbl, "sql": f"-- ERROR: Target {tbl} has no columns to select."})
            continue

        # --- Final CTAS SQL ---
                # New temp table name: <Target_Table>_<Source_Dataset_Id>_Temp
        temp_table_name = f"{tbl}_{source['Source_Dataset_Id']}_Temp"
        ctas = f"""
        CREATE OR REPLACE TABLE `{proj}.{ds}.{temp_table_name}` AS
        SELECT
            {projection}
        FROM `{source['Source_Project_Id']}.{source['Source_Dataset_Id']}.{source['Source_Table']}` AS `{source['Source_Table']}`
        {join_sql}
        """
        ctas_sqls.append({"table": temp_table_name, "sql": ctas.strip()})
    return ctas_sqls

    # --- CTAS SQL Preview Helper ---
    def generate_ctas_sqls(config_df: pd.DataFrame, selected_targets: Optional[List[str]] = None) -> List[str]:
        ctas_sqls = []
        if config_df.empty:
            return ctas_sqls
        # Filter targets
        if selected_targets:
            targets_df = config_df[config_df["Target_Table"].isin(selected_targets)]
        else:
            targets_df = config_df[config_df["Target_Table"].notnull()]
        # Group by target
        for (proj, ds, tbl), group in targets_df.groupby(["Target_Project_Id", "Target_Dataset_Id", "Target_Table"]):
            # Guardrail #1: must map to exactly one source
            sources = group[["Source_Project_Id", "Source_Dataset_Id", "Source_Table"]].drop_duplicates()
            if len(sources) != 1:
                ctas_sqls.append(f"-- ERROR: Target {tbl} maps to {len(sources)} sources, expected 1.")
                continue
            source = sources.iloc[0]
            # Guardrail #2: no duplicate target columns
            dup_cols = group["Target_Column"].duplicated().sum()
            if dup_cols > 0:
                ctas_sqls.append(f"-- ERROR: Target {tbl} has duplicate columns.")
                continue
            # Build projection
            group_sorted = group.sort_values(["Scenario_ID", "Target_Column"])
            projection = ",\n  ".join([
                f"{row['Derivation_Logic']} AS `{row['Target_Column']}" for _, row in group_sorted.iterrows()
            ])
            if not projection:
                ctas_sqls.append(f"-- ERROR: Target {tbl} has no columns to select.")
                continue
            # Build CTAS
            ctas = f"""
    CREATE OR REPLACE TABLE `{proj}.{ds}.{tbl}_Temp` AS\nSELECT\n  {projection}\nFROM `{source['Source_Project_Id']}.{source['Source_Dataset_Id']}.{source['Source_Table']}` AS `{source['Source_Table']}`
    """
            ctas_sqls.append(ctas.strip())
        return ctas_sqls

def call_build_proc(selected: Optional[List[str]]):
    client = get_bq_client()
    sql = f"CALL `{PROC_FQN}`(@cfg, @targets)"
    params = [
        ScalarQueryParameter("cfg", "STRING", TABLE_FQN),
        ArrayQueryParameter("targets", "STRING", selected if selected else []),
    ]
    job = client.query(sql, job_config=bigquery.QueryJobConfig(query_parameters=params))
    job.result()

# -----------------------------
# UI
# -----------------------------

# --- Streamlit UI setup ---
st.set_page_config(page_title="Validation Scenarios Manager", layout="wide")
st.title("Validation Scenarios Manager")
st.caption(f"Config table: `{TABLE_FQN}` · Procedure: `{PROC_FQN}`")

with st.expander("1) Upload or Update Config (Excel → BigQuery)", expanded=False):

    # Section 1: Upload Excel config to BigQuery
    uploaded = st.file_uploader("Upload Validation_Scenarios Excel (.xlsx)", type=["xlsx"])
    mode = st.radio("Upload mode", ["Recreate (Full Refresh)", "Append (Add New Rows)"], index=0, horizontal=True)
    if st.button("Upload to BigQuery", type="primary", disabled=uploaded is None):
        if not uploaded:
            st.error("Please upload an Excel file.")
        else:
            try:
                with st.spinner("Uploading..."):
                    load_excel_to_bq(uploaded.getvalue(), mode)
                st.success(f"Upload complete with mode: {mode}")
            except Exception as e:
                st.exception(e)

with st.expander("2) Components Selection", expanded=False):

    # Section 2: Select target tables and preview config
    try:
        components = list_components()
        select_all = st.checkbox("Select All", value=False)
        default = components if select_all else []
        selected = st.multiselect("Target tables", components, default=default)

        st.write("Preview (first 50 rows):")
        try:
            df_prev = preview_config(50)
            st.dataframe(df_prev, use_container_width=True, hide_index=True)
        except Exception:
            st.info("Config table preview not available yet.")
    except Exception:
        st.info("Config table not found. Upload the Excel first.")
        selected = []

with st.expander("3) Build Temp Tables", expanded=False):


    # Section 3: Preview SQL and run stored procedure
    try:
        # Preview the SQL that will be executed for each selected target
        config_df = preview_config(1000)  # get all config rows
        ctas_sqls = generate_ctas_sqls(config_df, selected)
        st.subheader("Preview: Create Table As Select SQL for Selected Targets")
        if ctas_sqls:
            for item in ctas_sqls:
                st.markdown(f"### Target Table: `{item['table']}`")
                st.code(item['sql'], language="sql")
        else:
            st.info("No SQL to preview. Check your selection or config table.")
    except Exception as e:
        st.warning(f"Could not generate SQL preview: {e}")

    # Show the stored procedure command that will be run
    st.subheader("Stored Procedure Command to be Executed")
    # Use markdown with code block for better wrapping and scrollability
    if selected:
        targets_str = ', '.join([f"'{t}'" for t in selected])
        targets_array = f"[{targets_str}]"
    else:
        targets_array = "NULL"
    proc_cmd = f"CALL `{PROC_FQN}`('{TABLE_FQN}', {targets_array});"
    st.markdown(f"""
<div style='overflow-x:auto; background:#f8f9fa; border-radius:8px; padding:8px;'>
<pre style='white-space:pre-wrap; word-break:break-all;'>
{proc_cmd}
</pre>
</div>
""", unsafe_allow_html=True)

    # Button to run the stored procedure
    if st.button("Run Stored Procedure", type="primary"):
        try:
            start = time.strftime("%Y-%m-%d %H:%M:%S")
            with st.spinner("Running stored procedure..."):
                call_build_proc(selected)
            end = time.strftime("%Y-%m-%d %H:%M:%S")
            st.success(f"Completed. Started: {start} · Finished: {end}")
            st.code(proc_cmd, language="sql")
        except Exception as e:
            st.exception(e)

# --- New expander for sample data preview ---
show_data = st.checkbox("Show 5 rows from each created temp table (after procedure run)")
if show_data and selected:
    with st.expander("Sample Data from Created Temp Tables", expanded=True):
        client = get_bq_client()
        config_df = preview_config(1000)
        ctas_sqls = generate_ctas_sqls(config_df, selected)
        for item in ctas_sqls:
            temp_table = item['table']
            try:
                # Extract base table and source dataset from temp table name
                if temp_table.endswith('_Temp'):
                    # New logic: match by constructing temp table name from config
                    # Removed debug/troubleshooting messages
                    match = config_df[
                        config_df.apply(
                            lambda row: f"{row['Target_Table']}_{row['Source_Dataset_Id']}_Temp" == temp_table,
                            axis=1
                        )
                    ]
                    if not match.empty:
                        proj = match.iloc[0]['Target_Project_Id']
                        ds = match.iloc[0]['Target_Dataset_Id']
                        temp_fqn = f"{proj}.{ds}.{temp_table}"
                        sql = f"SELECT * FROM `{temp_fqn}` LIMIT 5"
                        st.markdown(f"**{temp_fqn}**")
                        st.code(sql, language="sql")
                        df = client.query(sql).to_dataframe()
                        st.dataframe(df)
                    else:
                        st.warning(f"Could not find config for temp table {temp_table}")
                else:
                    st.warning(f"Temp table name does not end with _Temp: {temp_table}")
            except Exception as e:
                st.error(f"Error fetching data from {temp_table}: {e}")


# Footer note for authentication
st.divider()
st.caption("Note: Uses Application Default Credentials. Locally: `gcloud auth application-default login`.")
